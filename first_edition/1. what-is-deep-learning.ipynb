{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1- What is Deep Learning\n",
    "In classical programming, we would input both rules and data to get answers. However, in machine learning, we would input data and answers to get rules. Therefore, it is trained rather than explicitly programmed.\n",
    "\n",
    "## 1.1.1 - 1.1.3\n",
    "Machine Learning needs 3 things:\n",
    "Input data points, Examples of expected output, a way to measure whether the algorithm is doing a good job\n",
    "\n",
    "Machine learning transforms input to meaningful output through examples of known input and output. To learn useful **representation** of the input that leads us closer to expected output\n",
    "\n",
    "Example is if we have a graph that classifies data, we can change the coordinates so that everything above 0 in the X axis is one classification while below 0 is another. We can measure how well the algorithm does by percentage of points correctly classified. **Learning** in ML describes automatic search process for better representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 The \"deep\" in deep learning\n",
    "Deep in deep learning stands for this idea of successive layers of representation. The number of layers is called the **depth**. This is almost always done via **neural networks**. Overall, it is a multistage way to learn data representation.\n",
    "\n",
    "## 1.1.5 Understanding how deep learning works in 3 figures\n",
    "Transformation implemented by later is parameterized by its weights which is a bunch of numbers\n",
    "\n",
    "In this context, learning means finding a set of values for weights of all layers in a networks such that the network will correctly map example inputs to their associated targets.\n",
    "\n",
    "**Loss function/Objective function** takes predictions of the network and target and computers distances score (how well the model did). It is the feedback signal to adjust the weight to lower loss score\n",
    "\n",
    "The optimizer does this by implementing the **Backpropagation Algorithm**\n",
    "\n",
    "This loop is the **Training loop** and it repeat a number of times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Probabilistic modelling\n",
    "Probabilistic modelling is the application of statistics to data analysis. **Naive Bayes Algorithm** is best known and it is a classifier based algorithm. It applies Bayes' theorem assuming the features in input data is independent.\n",
    "\n",
    "**Logistic regression** is a similar model. Despite the name, it is not a regression algorithm but it is a classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Kernal Methods\n",
    "**Kernal methods** is a new approach to neural nets that are a group of classification algorithms. The most popular of which is support vector machine (SVM)\n",
    "\n",
    "**SVM** solves classification problems by finding decision boundries, a line or surface separating training data into two spaces corresponding to 2 categories. A good decision boundry maximizes distance between hyerplane and closest data point from each class (maximizing the margains.)\n",
    "\n",
    "**Kernal trick** computes the distance between pairs of points in that space which is done via kernal function. **kernal function** maps any two points in initial space to the distance between these points in target representation space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Decision Trees, Random Forest, and Gradient Boosting Machines.\n",
    "**Decision Tree** classify input data points or predict output values given inputs\n",
    "\n",
    "**Random Forest** builds large number of specialized decision trees and combining their outputs\n",
    "\n",
    "**Gradient Boosting Machine** combines weak prediction models, generally decision trees. It uses gradient boosting to interatively train new models that specializes in addressing weak points of previous model. May be considered the best, if not best, algorithm for non preceptual data.\n",
    "\n",
    "In 2012, deep convolutional neural networks became the go to for all perceptual tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.6 What makes Deep Learning Different?\n",
    "Deep Learning offered better performence and mainly, easier problem solving because it automates feature engineering (engineer layers of representation for their data)\n",
    "\n",
    "***Two characteristics how deep learning learns from data***\n",
    "- Incemental layer by layer way in which increasingly complex representations are developed\n",
    "- These incremental representations are learned jointly (layers update with needs of both above and below)\n",
    "\n",
    "**OVERALL** use graidient boosting machines for shallow learning problems (XGBoost library) and deep learning for perceptual problem (Keras library).\n",
    "\n",
    "Deep Learning offers:\n",
    "- No feature engineering\n",
    "- Can be trained for any size data\n",
    "- can be reused and versitile (add more unique data on top of it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
